{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b39065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "345c3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbroadcast(grad, original_shape):\n",
    "    # gradient has more dims than original \n",
    "    # this is needed bcoz numpy automatically broadcasts the biases to the shape of the output\n",
    "    # but the gradient is the sum over the broadcasted dimensions\n",
    "    # sp we shrink the gradient back to the original shape by summing over the broadcasted dimensions\n",
    "    while grad.ndim > len(original_shape):\n",
    "        grad = grad.sum(axis=0) # sum over added dimensions\n",
    "    # dimensions match but lengths differ \n",
    "    for axis, (dim_grad, dim_orig) in enumerate(zip(grad.shape, original_shape)):\n",
    "        if dim_orig == 1 and dim_grad > 1:\n",
    "            grad = grad.sum(axis=axis, keepdims=True) # sum over broadcasted dimensions\n",
    "    return grad\n",
    "\n",
    "# ------------------------------------------------\n",
    "class Config:\n",
    "    # global switch for enabling/disabling gradient tracking\n",
    "    enable_grad = True\n",
    "\n",
    "class no_grad:\n",
    "    def __enter__(self):\n",
    "        # is run when user writes  \"with no_grad():\"\n",
    "        self.prev = Config.enable_grad\n",
    "        Config.enable_grad = False\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        # is run at the end of the with block\n",
    "        Config.enable_grad = self.prev\n",
    "# ------------------------------------------------\n",
    "class Context:\n",
    "    # save information for the backward pass used in Function.forward to be used in Function.backward\n",
    "    def __init__(self):\n",
    "        self.saved_tensors = ()\n",
    "        \n",
    "    def save_for_backward(self, *args):\n",
    "        self.saved_tensors = args\n",
    "\n",
    "class Function:\n",
    "    # class for defining mathematical operations\n",
    "    @staticmethod\n",
    "    def forward(context, *args):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(context, *grad_outputs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def apply(self_class, *args):\n",
    "        context = Context() # create the individual context to store info for backward pass\n",
    "        operation_arguments = [t.data if isinstance(t, Tensor) else t for t in args] # get data not tensors\n",
    "        \n",
    "        output_data = self_class.forward(context, *operation_arguments) # call forward\n",
    "        parents = [t for t in args if isinstance(t, Tensor)] # get the tensors with which this operation is called\n",
    "        requires_grad = any(p.req_grad for p in parents) # if any I do too xdd\n",
    "        out = Tensor(output_data, _parents=tuple(parents), req_grad=requires_grad) # create output tensor\n",
    "        if requires_grad and Config.enable_grad:\n",
    "            def _backward():\n",
    "                grad_output = out.gradient # this is the grad coming from the next node in the graph - we calculate backwards xddd\n",
    "                grads = self_class.backward(context, grad_output) # calculate gradients wrt inputs\n",
    "                if not isinstance(grads, tuple):\n",
    "                    grads = (grads, )\n",
    "                for parent, grad in zip(parents, grads):\n",
    "                    if parent.req_grad:\n",
    "                        parent.gradient += grad # pass the gradient to the parents - backward \n",
    "            out._backward = _backward # this is not called yet, only assigned - will be called during Tensor.backward()\n",
    "        return out\n",
    "# ------------------------------------------------\n",
    "class Add(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, self_tensor_data, other_tensor_data):\n",
    "        context.save_for_backward(self_tensor_data.shape, other_tensor_data.shape) # save original shapes for backward\n",
    "        return self_tensor_data + other_tensor_data # simple add\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        shape_x, shape_y = context.saved_tensors\n",
    "        grad_x = unbroadcast(grad_output, shape_x) # the derivate of addition is 1, but we need to unbroadcast\n",
    "        grad_y = unbroadcast(grad_output, shape_y)\n",
    "        return grad_x, grad_y\n",
    "\n",
    "\n",
    "class MatMul(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, self_tensor_data, other_tensor_data):\n",
    "        context.save_for_backward(self_tensor_data, other_tensor_data)\n",
    "        return self_tensor_data @ other_tensor_data # matrix multiplication\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        a, b = context.saved_tensors\n",
    "        \n",
    "        grad_a = grad_output @ b.T # derivative is b.T\n",
    "        grad_b = a.T @ grad_output\n",
    "        return grad_a, grad_b\n",
    "class Substract(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, self_tensor_data, other_tensor_data):\n",
    "        context.save_for_backward(self_tensor_data.shape, other_tensor_data.shape) \n",
    "        return self_tensor_data - other_tensor_data\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        shape_x, shape_y = context.saved_tensors\n",
    "        grad_x = unbroadcast(grad_output, shape_x)\n",
    "        grad_y = unbroadcast(-grad_output, shape_y) # derivative of subtraction is -1 for the second input\n",
    "        return grad_x, grad_y\n",
    "class Multiply(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, self_tensor_data, other_tensor_data):\n",
    "        context.save_for_backward(self_tensor_data, other_tensor_data) \n",
    "        return self_tensor_data * other_tensor_data\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        a, b = context.saved_tensors\n",
    "        grad_a = grad_output * b # derivative is b\n",
    "        grad_b = grad_output * a\n",
    "        if a.shape != b.shape:\n",
    "            grad_a = unbroadcast(grad_a, a.shape) # again check if np broadcasted - we need to keep original shapes for gradients\n",
    "            grad_b = unbroadcast(grad_b, b.shape)\n",
    "        return grad_a, grad_b\n",
    "class Sum(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, input_data, axis=None, keepdims=False):\n",
    "        context.save_for_backward(input_data.shape, axis, keepdims) \n",
    "        return np.sum(input_data, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        original_shape, axis, keepdims = context.saved_tensors\n",
    "        if axis is None:\n",
    "            grad_input = np.ones(original_shape) * grad_output\n",
    "        else:\n",
    "            if not keepdims:\n",
    "                grad_output = np.expand_dims(grad_output, axis)\n",
    "            grad_input = np.ones(original_shape) * grad_output\n",
    "        return grad_input\n",
    "class Pow(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, self_tensor_data, other_tensor_data):\n",
    "        context.save_for_backward(self_tensor_data, other_tensor_data)\n",
    "        return self_tensor_data ** other_tensor_data\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        a, b = context.saved_tensors\n",
    "        return grad_output * b * (a ** (b - 1)), None\n",
    "class ReLU(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, input_data):\n",
    "        context.save_for_backward(input_data)\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        (input_data,) = context.saved_tensors\n",
    "        grad_input = grad_output * (input_data > 0).astype(input_data.dtype) # derivative of ReLU is 1 for positive inputs, else 0\n",
    "        return grad_input\n",
    "class T(Function):\n",
    "    @staticmethod\n",
    "    def forward(context, input_data):\n",
    "        context.save_for_backward(input_data)\n",
    "        return input_data.T\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        (input_data,) = context.saved_tensors\n",
    "        grad_input = grad_output.T\n",
    "        return grad_input\n",
    "# ------------------------------------------------\n",
    "class Tensor:\n",
    "    def __init__(self, data, _parents=(),req_grad=False):\n",
    "        self.data = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "        self.gradient = np.zeros(self.data.shape)\n",
    "        self.req_grad = req_grad\n",
    "        self._parents = _parents\n",
    "        self._backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"d = {self.data}, g = {self.gradient}\"\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        # Stack holds tuples: (node, children_pushed_to_stack_flag)\n",
    "        stack = [(self, False)] \n",
    "        \n",
    "        while stack:\n",
    "            node, children_pushed = stack.pop()\n",
    "            \n",
    "            if node in visited:\n",
    "                continue\n",
    "                \n",
    "            if children_pushed:\n",
    "                # If we are seeing this node for the second time, \n",
    "                # it means we have finished processing its children.\n",
    "                visited.add(node)\n",
    "                topo.append(node)\n",
    "            else:\n",
    "                # First time seeing this node:\n",
    "                # 1. Put it back on the stack marked as \"children pushed\"\n",
    "                stack.append((node, True))\n",
    "                \n",
    "                # 2. Push all its children onto the stack\n",
    "                for child in node._parents:\n",
    "                    if child not in visited:\n",
    "                        stack.append((child, False))\n",
    "        \n",
    "        # Proceed with the chain rule as before\n",
    "        self.gradient = np.ones_like(self.data, dtype=float)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Add.apply(self, other) # call the static method apply of Add which does both forward and backward\n",
    "    def __radd__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Add.apply(other, self)\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return MatMul.apply(self, other)\n",
    "    def __rmul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return MatMul.apply(other, self)\n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Substract.apply(self, other)\n",
    "    def __rsub__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Substract.apply(other, self)\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Multiply.apply(self, other)\n",
    "    def __rmul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Multiply.apply(other, self)\n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Pow.apply(self, other)\n",
    "    def relu(self):\n",
    "        return ReLU.apply(self)\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return Sum.apply(self, axis, keepdims)\n",
    "    @property\n",
    "    def T(self):\n",
    "        return T.apply(self)\n",
    "    \n",
    "        \n",
    "\n",
    "# ------------------------------------------------\n",
    "class Parameter(Tensor):\n",
    "    # just a tensor that wants gradients\n",
    "    # the weights and biases are this class\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.req_grad = True # if it is a parameter, we always need gradients\n",
    "\n",
    "class Module:\n",
    "    # this is a bit scatchy u might understand it better than me xd\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "\n",
    "    def __setattr__(self, name, value): # this is run when we do \"self.name = value\" in the Tensor init func\n",
    "        # if we are setting a Parameter (tensor with gradients), save it to _parameters\n",
    "        if isinstance(value, Parameter):\n",
    "            self._parameters[name] = value\n",
    "        # if we are setting a sub-Module, save it to _modules\n",
    "        elif isinstance(value, Module):\n",
    "            self._modules[name] = value\n",
    "        \n",
    "        # do the default behavior (actually set the attribute)\n",
    "        object.__setattr__(self, name, value)\n",
    "\n",
    "    def parameters(self):\n",
    "        # recursively find all parameters\n",
    "        # this is called in the training loop the get all the tensor so that we can push their data wrt their gradients!\n",
    "        params = list(self._parameters.values())\n",
    "        for module in self._modules.values():\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "# ------------------------------------------------\n",
    "class Optimizer():\n",
    "    # default optimizer class\n",
    "    def __init__(self, parameters, lr=0.01, weight_decay=0.0, clip_norm=1.0):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.clip_norm = clip_norm\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.gradient = np.zeros_like(p.data)\n",
    "            \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, parameters, lr=0.01, weight_decay=0.0, clip_norm=1.0):\n",
    "        super().__init__(parameters, lr=lr, weight_decay=weight_decay, clip_norm=clip_norm)\n",
    "    def step(self):\n",
    "        if self.clip_norm is not None:\n",
    "            total_norm = 0\n",
    "            for p in self.parameters:\n",
    "                if p.req_grad:\n",
    "                    total_norm += np.sum(p.gradient ** 2)\n",
    "            total_norm = np.sqrt(total_norm)\n",
    "            \n",
    "            clip_coef = self.clip_norm / (total_norm + 1e-6)\n",
    "            if clip_coef < 1:\n",
    "                for p in self.parameters:\n",
    "                    if p.req_grad:\n",
    "                        p.gradient *= clip_coef\n",
    "\n",
    "        for p in self.parameters:\n",
    "            if p.req_grad:\n",
    "                grad = p.gradient\n",
    "                \n",
    "                if self.weight_decay > 0:\n",
    "                    grad = p.gradient + self.weight_decay * p.data\n",
    "                \n",
    "                p.data -= self.lr * grad\n",
    "# ------------------------------------------------\n",
    "class Criterion():\n",
    "    # default loss class\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, predictions, targets):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class MSELoss(Criterion):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self, predictions, targets):\n",
    "        diff = predictions - targets\n",
    "        return (diff ** 2).sum() * (1.0 / predictions.data.shape[0])\n",
    "# ------------------------------------------------\n",
    "class Neuron(Module):\n",
    "    # NOT used in MLP xd\n",
    "    def __init__(self, input_size, nonlinearity=True):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng()\n",
    "        self.w = Parameter(rng.standard_normal((input_size, 1)) * (1.0 / np.sqrt(input_size)))\n",
    "        self.b = Parameter(np.zeros((1, 1)))\n",
    "        self.nonlinearity = nonlinearity\n",
    "        \n",
    "    def forward(self, x):\n",
    "        act = self.w.T @ x + self.b\n",
    "        return act.relu() if self.nonlinearity else act\n",
    "    \n",
    "    def __call__(self, x): # so that we can call neuron like a function\n",
    "        return self.forward(x)\n",
    "# ------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, input_size, output_size, init_method=\"glorot\"):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng()\n",
    "        if init_method == \"glorot\":\n",
    "            scale = np.sqrt(2.0 / (input_size + output_size))\n",
    "            self.w = Parameter(rng.standard_normal((output_size, input_size)) * scale)\n",
    "        elif init_method == \"he\":\n",
    "            scale = np.sqrt(2.0 / input_size)\n",
    "            self.w = Parameter(rng.standard_normal((output_size, input_size)) * scale)\n",
    "        elif init_method == \"zero\":\n",
    "            self.w = Parameter(np.zeros((output_size, input_size)))\n",
    "        else:\n",
    "            self.w = Parameter(rng.standard_normal((output_size, input_size)) * (1.0 / np.sqrt(input_size)))\n",
    "        # yes it is initialized transposed xd its bxoz we do x @ w.T in forward\n",
    "        self.b = Parameter(np.zeros((output_size, 1))) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.w.T + self.b.T # yay dims  \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x) # pytorch style calling B)\n",
    "    \n",
    "class MLP(Module):\n",
    "    def __init__(self, n_in, n_outs, init_method=\"glorot\"):\n",
    "        # nin is input size (3), n_outs is a list of output sizes of each layer [4,4,1]\n",
    "        super().__init__()\n",
    "        sz = [n_in] + n_outs # this is now [3,4,4,1] xd\n",
    "        self.layers_list = []\n",
    "        for i in range(len(n_outs)):\n",
    "            layer = Linear(sz[i], sz[i+1], init_method=init_method)\n",
    "            name = f\"layer_{i}\"\n",
    "            setattr(self, name, layer) # this will also add it to _modules via __setattr__ so that parameters() works.. recursion baby\n",
    "            self.layers_list.append(layer) # keep a list of layers for forward pass\n",
    "\n",
    "    def forward(self, x, activation=\"relu\"):\n",
    "        for i, layer in enumerate(self.layers_list):\n",
    "            x = layer(x) # this calls the forward of Linear\n",
    "            if i != len(self.layers_list) - 1:\n",
    "                if activation == \"relu\":\n",
    "                    x = x.relu()\n",
    "                # elif activation == \"sigmoid\":\n",
    "                #     x = x.sigmoid()\n",
    "                # elif activation == \"tanh\":\n",
    "                #     x = x.tanh() \n",
    "        return x\n",
    "        \n",
    "    def __call__(self, x, activation=\"relu\"):\n",
    "        return self.forward(x, activation=activation) \n",
    "# ------------------------------------------------\n",
    "\n",
    "# class with input same as the instructions say - FFNN class with the following configurable hyperparameters:Num epochs, num_hidden_layers, n_hidden_units, learning rate, optimizer, batch_size, l2_coeff, weights_init, activation, loss etc.\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self,num_epochs=15, num_hidden_layers=3, num_hidden_units=[64, 64, 32], learning_rate=0.01, optimizer=\"sgd\", batch_size=16, l2_coeff=0.01, weights_init=\"he\", activation=\"relu\", criterion=\"mse\", clip_norm=1.0):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_hidden_units = num_hidden_units if isinstance(num_hidden_units, list) else [num_hidden_units]*num_hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_coeff = l2_coeff\n",
    "        self.weights_init = weights_init\n",
    "        self.activation = activation\n",
    "        self.clip_norm = clip_norm\n",
    "        criterion_classes = {\"mse\": MSELoss,\n",
    "                             # \"cross_entropy\": CrossEntropyLoss, # if you implement more losses, add them here\n",
    "                            }\n",
    "        optimizer_classes = {\"sgd\": SGD,\n",
    "                             # \"adam\": Adam, # if you implement more optimizers, add them here\n",
    "                             # \"adamW\": AdamW\n",
    "                            }\n",
    "        self.optimizer_class = optimizer_classes[optimizer]\n",
    "        self.criterion = criterion_classes[criterion]()\n",
    "        self.model = None\n",
    "        \n",
    "    def create_model(self, input_size, output_size):\n",
    "        n_outs = self.num_hidden_units + [output_size]\n",
    "        self.model = MLP(input_size, n_outs, init_method=self.weights_init)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        num_samples = x_train.shape[0]\n",
    "        \n",
    "        optimizer = self.optimizer_class(self.model.parameters(), lr=self.learning_rate, weight_decay=self.l2_coeff, clip_norm=self.clip_norm)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                x_batch = Tensor(x_train[start:end])\n",
    "                y_batch = Tensor(y_train[start:end])\n",
    "                \n",
    "                preds = self.model(x_batch, activation=self.activation)\n",
    "                loss = self.criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.data.sum()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss / (num_samples // self.batch_size)}\")\n",
    "            \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # lets test if no_grad works properly\n",
    "        print(\"Evaluating on test data...\")\n",
    "        \n",
    "        with no_grad():\n",
    "            inputs = Tensor(x_test)\n",
    "            targets = Tensor(y_test)\n",
    "            preds = self.model(inputs)\n",
    "            loss = self.criterion(preds, targets)\n",
    "            print(f\"Test Loss: {loss.data}\")\n",
    "            accuracy = np.mean(np.argmax(preds.data, axis=1) == np.argmax(targets.data, axis=1))\n",
    "            print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "            # lets print some gradients to see if they are zero\n",
    "            # for i, param in enumerate(self.model.parameters()):\n",
    "            #     print(f\"Param {i} gradient norm: {np.linalg.norm(param.gradient)}\")\n",
    "            return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10867079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/zalando-research/fashionmnist?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68.8M/68.8M [00:06<00:00, 11.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\onder\\.cache\\kagglehub\\datasets\\zalando-research\\fashionmnist\\versions\\4\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"zalando-research/fashionmnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0003aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion dataset\n",
    "train = np.loadtxt(\"datasets/fashion-mnist_train.csv\", delimiter=\",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa818aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"datasets/fashion-mnist_test.csv\", delimiter=\",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ec48ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = train[:, 0]  \n",
    "x_train = train[:, 1:] \n",
    "x_test = test[:, 1:]  \n",
    "y_test = test[:, 0]  \n",
    "num_samples = x_train.shape[0]\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1c71869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3778916413918811\n",
      "Epoch 2, Loss: 0.2535041091381651\n",
      "Epoch 3, Loss: 0.22527715843316912\n",
      "Epoch 4, Loss: 0.20835588933130214\n",
      "Epoch 5, Loss: 0.1961701463113599\n",
      "Epoch 6, Loss: 0.18653033508613923\n",
      "Epoch 7, Loss: 0.1786826589480956\n",
      "Epoch 8, Loss: 0.17199178076467486\n",
      "Epoch 9, Loss: 0.16627177285889277\n",
      "Epoch 10, Loss: 0.16121488694576402\n",
      "Epoch 11, Loss: 0.1563315042780901\n",
      "Epoch 12, Loss: 0.15193136836438445\n",
      "Epoch 13, Loss: 0.14799294422805515\n",
      "Epoch 14, Loss: 0.14432830646405742\n",
      "Epoch 15, Loss: 0.141041985588635\n",
      "Epoch 16, Loss: 0.13778136671241326\n",
      "Epoch 17, Loss: 0.13492728429335335\n",
      "Epoch 18, Loss: 0.13193884986794016\n",
      "Epoch 19, Loss: 0.12928411289616276\n",
      "Epoch 20, Loss: 0.12673803685025006\n",
      "Epoch 21, Loss: 0.12438418855342268\n",
      "Epoch 22, Loss: 0.12195137758224715\n",
      "Epoch 23, Loss: 0.11950938772288562\n",
      "Epoch 24, Loss: 0.11724404659950333\n",
      "Epoch 25, Loss: 0.11480150027095191\n",
      "Epoch 26, Loss: 0.1128677761791242\n",
      "Epoch 27, Loss: 0.1109359122346305\n",
      "Epoch 28, Loss: 0.10930572326186613\n",
      "Epoch 29, Loss: 0.10677128762799015\n",
      "Epoch 30, Loss: 0.10501917129815969\n",
      "Evaluating on test data...\n",
      "Test Loss: 0.18159098952026279\n",
      "Test Accuracy: 89.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.18159099)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same with FFNN class\n",
    "ffnn = FFNN(num_epochs=30, num_hidden_layers=2, num_hidden_units=[256, 128], learning_rate=0.1, optimizer=\"sgd\", batch_size=64, l2_coeff=0.00001, weights_init=\"he\", activation=\"relu\", criterion=\"mse\", clip_norm=1.0)\n",
    "ffnn.create_model(input_size=784, output_size=10)\n",
    "# normalize data\n",
    "x_train_normalized = x_train / 255.0\n",
    "y_train_onehot = np.eye(10)[y_train.astype(int)]\n",
    "x_test_normalized = x_test / 255.0\n",
    "y_test_onehot = np.eye(10)[y_test.astype(int)]\n",
    "ffnn.train(x_train_normalized, y_train_onehot)\n",
    "ffnn.evaluate(x_test_normalized, y_test_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
